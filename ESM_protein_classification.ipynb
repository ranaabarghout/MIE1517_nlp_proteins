{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESM_protein_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## Installlation and imports"
      ],
      "metadata": {
        "id": "w3cPrEyCGc5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `esm` (https://github.com/facebookresearch/esm)"
      ],
      "metadata": {
        "id": "sD_kQxa5GkCg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6a6B8LTGBxu",
        "outputId": "318e812a-19ab-4668-d5f0-2fbb26be38f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fair-esm\n",
            "  Downloading fair_esm-0.4.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: fair-esm\n",
            "Successfully installed fair-esm-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Import libraries"
      ],
      "metadata": {
        "id": "CaQV2aSKKd5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import esm\n",
        "import time"
      ],
      "metadata": {
        "id": "2f6a1-J6GaUB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "_Wx62it7Undx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "QMmTIsGlXcSQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Init pre-trained ESM model and alphabet (this may take about 5min)"
      ],
      "metadata": {
        "id": "von16_DpKgIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# esm_model, alphabet = torch.hub.load(\n",
        "#     \"facebookresearch/esm:main\", \n",
        "#     \"esm1b_t33_650M_UR50S\"\n",
        "# )"
      ],
      "metadata": {
        "id": "sreipGDhOGkp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, alphabet = esm.pretrained.esm.pretrained.esm1b_t33_650M_UR50S()"
      ],
      "metadata": {
        "id": "Dp8RzxxpAUND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Init batch converter"
      ],
      "metadata": {
        "id": "8zX8VfLPOTio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_converter = alphabet.get_batch_converter()"
      ],
      "metadata": {
        "id": "PWyypq5yORzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## Load and pre-process data"
      ],
      "metadata": {
        "id": "CSK-R51VGwMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_data = pd.read_csv(\n",
        "    'results.csv', \n",
        "    skiprows = 1, \n",
        "    names = ['ECnumber', 'Sequence', 'Specimen']\n",
        ")\n",
        "print(seq_data.shape)"
      ],
      "metadata": {
        "id": "VivQ3XgnIDeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Perform data pre-processing"
      ],
      "metadata": {
        "id": "2hIB36dIXSi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_data = seq_data[seq_data['Sequence'].apply(len) <= 1000]"
      ],
      "metadata": {
        "id": "5ClTK-CyQ4Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ec_encoder = LabelEncoder()\n",
        "seq_data['EClabel'] = ec_encoder.fit_transform(seq_data['ECnumber'])"
      ],
      "metadata": {
        "id": "CIizvqLJXYAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(seq_data.shape)"
      ],
      "metadata": {
        "id": "8uIS92wrXz_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_data.head()"
      ],
      "metadata": {
        "id": "zVyyWJ9UIaQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## Create dataset"
      ],
      "metadata": {
        "id": "hFPbMnNKc2P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class protDataset(Dataset):\n",
        "    def __init__(self, labels, sequences, tokens):\n",
        "        super().__init__()\n",
        "        assert len(labels) == len(sequences)\n",
        "        assert len(labels) == tokens.shape[0]\n",
        "        \n",
        "        # Add dataset size\n",
        "        self.n = len(labels)\n",
        "\n",
        "        # Add labels and token to class\n",
        "        self.labels = torch.as_tensor(labels)\n",
        "        self.tokens = tokens\n",
        "\n",
        "        # Get sequences lengths\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        self.lengths = torch.as_tensor(lengths)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.labels[idx], self.lengths[idx], self.tokens[idx,:]"
      ],
      "metadata": {
        "id": "9XmUl0cXUUvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['EClabel', 'Sequence']\n",
        "seq_data_list = seq_data[cols].values.tolist()\n",
        "labels, sequences, tokens = batch_converter(seq_data_list)\n",
        "dataset = protDataset(labels, sequences, tokens)"
      ],
      "metadata": {
        "id": "8sgzKGAMLBTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "FsJeqOrQfUE0",
        "outputId": "917321b6-9ebb-4bbb-f59c-3219c64b1450"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4b9f1d7fa9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## Get sequence representations"
      ],
      "metadata": {
        "id": "L0_tVF4sJcVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sequenceEmbedder():\n",
        "    def __init__(self, model, dataset, batch_size = 16, num_layers = 33, device = 'cuda'):\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Initiate data loader\n",
        "        self.loader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=self.batch_size,\n",
        "            shuffle = False\n",
        "        )\n",
        "        \n",
        "        # Set to eval model\n",
        "        self.model.eval()\n",
        "\n",
        "        # Set device\n",
        "        if self.device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "            self.model.cuda()\n",
        "        else:\n",
        "            self.model.cpu()\n",
        "\n",
        "    def average_tokens(self, lengths, representations):\n",
        "        seq_reps = []\n",
        "        for i, l in enumerate(lengths):\n",
        "            k = l + 1\n",
        "            representation = representations[i]           \n",
        "            r = representation[1:k,:].mean(0)\n",
        "            seq_reps.append(r)\n",
        "        seq_reps = torch.stack(seq_reps)\n",
        "        return seq_reps\n",
        "\n",
        "    def get_embeddings(self, verbosity = 100):\n",
        "        # Init containers         \n",
        "        embeddings = [] \n",
        "        all_labels = []        \n",
        "        with torch.no_grad():\n",
        "            start = time.time()\n",
        "            for i, (labels, lengths, tokens) in enumerate(self.loader):\n",
        "                # Set device\n",
        "                if self.device == 'cuda':\n",
        "                    tokens = tokens.cuda()\n",
        "\n",
        "                # Get ESM results\n",
        "                res = self.model(\n",
        "                    tokens, \n",
        "                    repr_layers = [self.num_layers], \n",
        "                    return_contacts = False\n",
        "                )\n",
        "\n",
        "                # Extract token representations\n",
        "                token_representations = res['representations'][self.num_layers]\n",
        "\n",
        "                # Average across tokens\n",
        "                sequence_representations = self.average_tokens(lengths, token_representations)\n",
        "\n",
        "                # Detach and bring to cpu (unload gpu)\n",
        "                sequence_representations = sequence_representations.detach().cpu()\n",
        "\n",
        "                # Add to containers\n",
        "                embeddings.append(sequence_representations)\n",
        "                all_labels.append(labels)\n",
        "\n",
        "                if i % verbosity == 0:\n",
        "                    print('Batch no. {}; Sequence no.: {}; Elapsed time: {:1.2f}'.format(i + 1, (i + 1) * self.batch_size, time.time() - start))\n",
        "\n",
        "            # Concat\n",
        "            embeddings = torch.cat(embeddings)\n",
        "            all_labels = torch.cat(all_labels)\n",
        "\n",
        "        return embeddings, all_labels\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L9yRUbuxRVZj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = sequenceEmbedder(esm_model, dataset, batch_size = 16, device = 'cuda')"
      ],
      "metadata": {
        "id": "X1vP4q1PTSsi"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embedder.get_embeddings(verbosity = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7QvOn2idA-O",
        "outputId": "cf991cd0-79fd-4597-b463-d34d109a758d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch no. 1; Sequence no.: 16; Elapsed time: 2.91\n",
            "Batch no. 6; Sequence no.: 96; Elapsed time: 82.02\n",
            "Batch no. 11; Sequence no.: 176; Elapsed time: 161.34\n",
            "Batch no. 16; Sequence no.: 256; Elapsed time: 240.69\n",
            "Batch no. 21; Sequence no.: 336; Elapsed time: 319.22\n",
            "Batch no. 26; Sequence no.: 416; Elapsed time: 398.52\n",
            "Batch no. 31; Sequence no.: 496; Elapsed time: 477.90\n",
            "Batch no. 36; Sequence no.: 576; Elapsed time: 557.26\n",
            "Batch no. 41; Sequence no.: 656; Elapsed time: 636.58\n",
            "Batch no. 46; Sequence no.: 736; Elapsed time: 715.84\n",
            "Batch no. 51; Sequence no.: 816; Elapsed time: 795.24\n",
            "Batch no. 56; Sequence no.: 896; Elapsed time: 874.53\n",
            "Batch no. 61; Sequence no.: 976; Elapsed time: 953.91\n",
            "Batch no. 66; Sequence no.: 1056; Elapsed time: 1033.08\n",
            "Batch no. 71; Sequence no.: 1136; Elapsed time: 1111.26\n",
            "Batch no. 76; Sequence no.: 1216; Elapsed time: 1189.85\n",
            "Batch no. 81; Sequence no.: 1296; Elapsed time: 1268.86\n",
            "Batch no. 86; Sequence no.: 1376; Elapsed time: 1348.09\n",
            "Batch no. 91; Sequence no.: 1456; Elapsed time: 1427.06\n",
            "Batch no. 96; Sequence no.: 1536; Elapsed time: 1505.56\n",
            "Batch no. 101; Sequence no.: 1616; Elapsed time: 1584.92\n",
            "Batch no. 106; Sequence no.: 1696; Elapsed time: 1663.86\n",
            "Batch no. 111; Sequence no.: 1776; Elapsed time: 1743.05\n",
            "Batch no. 116; Sequence no.: 1856; Elapsed time: 1822.26\n",
            "Batch no. 121; Sequence no.: 1936; Elapsed time: 1901.32\n",
            "Batch no. 126; Sequence no.: 2016; Elapsed time: 1980.54\n",
            "Batch no. 131; Sequence no.: 2096; Elapsed time: 2059.20\n",
            "Batch no. 136; Sequence no.: 2176; Elapsed time: 2138.44\n",
            "Batch no. 141; Sequence no.: 2256; Elapsed time: 2217.98\n",
            "Batch no. 146; Sequence no.: 2336; Elapsed time: 2297.00\n",
            "Batch no. 151; Sequence no.: 2416; Elapsed time: 2375.44\n",
            "Batch no. 156; Sequence no.: 2496; Elapsed time: 2454.82\n",
            "Batch no. 161; Sequence no.: 2576; Elapsed time: 2534.00\n",
            "Batch no. 166; Sequence no.: 2656; Elapsed time: 2612.87\n",
            "Batch no. 171; Sequence no.: 2736; Elapsed time: 2691.45\n",
            "Batch no. 176; Sequence no.: 2816; Elapsed time: 2770.76\n",
            "Batch no. 181; Sequence no.: 2896; Elapsed time: 2850.32\n",
            "Batch no. 186; Sequence no.: 2976; Elapsed time: 2929.78\n",
            "Batch no. 191; Sequence no.: 3056; Elapsed time: 3009.07\n",
            "Batch no. 196; Sequence no.: 3136; Elapsed time: 3088.02\n",
            "Batch no. 201; Sequence no.: 3216; Elapsed time: 3167.12\n",
            "Batch no. 206; Sequence no.: 3296; Elapsed time: 3246.33\n",
            "Batch no. 211; Sequence no.: 3376; Elapsed time: 3325.43\n",
            "Batch no. 216; Sequence no.: 3456; Elapsed time: 3404.83\n",
            "Batch no. 221; Sequence no.: 3536; Elapsed time: 3484.04\n",
            "Batch no. 226; Sequence no.: 3616; Elapsed time: 3563.36\n",
            "Batch no. 231; Sequence no.: 3696; Elapsed time: 3642.13\n",
            "Batch no. 236; Sequence no.: 3776; Elapsed time: 3720.95\n",
            "Batch no. 241; Sequence no.: 3856; Elapsed time: 3799.99\n",
            "Batch no. 246; Sequence no.: 3936; Elapsed time: 3879.18\n",
            "Batch no. 251; Sequence no.: 4016; Elapsed time: 3958.54\n",
            "Batch no. 256; Sequence no.: 4096; Elapsed time: 4037.89\n",
            "Batch no. 261; Sequence no.: 4176; Elapsed time: 4117.19\n",
            "Batch no. 266; Sequence no.: 4256; Elapsed time: 4196.28\n",
            "Batch no. 271; Sequence no.: 4336; Elapsed time: 4275.09\n",
            "Batch no. 276; Sequence no.: 4416; Elapsed time: 4354.07\n",
            "Batch no. 281; Sequence no.: 4496; Elapsed time: 4433.11\n",
            "Batch no. 286; Sequence no.: 4576; Elapsed time: 4512.62\n",
            "Batch no. 291; Sequence no.: 4656; Elapsed time: 4590.96\n",
            "Batch no. 296; Sequence no.: 4736; Elapsed time: 4670.29\n",
            "Batch no. 301; Sequence no.: 4816; Elapsed time: 4749.48\n",
            "Batch no. 306; Sequence no.: 4896; Elapsed time: 4829.40\n",
            "Batch no. 311; Sequence no.: 4976; Elapsed time: 4907.56\n",
            "Batch no. 316; Sequence no.: 5056; Elapsed time: 4986.34\n",
            "Batch no. 321; Sequence no.: 5136; Elapsed time: 5065.53\n",
            "Batch no. 326; Sequence no.: 5216; Elapsed time: 5144.81\n",
            "Batch no. 331; Sequence no.: 5296; Elapsed time: 5223.76\n",
            "Batch no. 336; Sequence no.: 5376; Elapsed time: 5302.50\n",
            "Batch no. 341; Sequence no.: 5456; Elapsed time: 5381.58\n",
            "Batch no. 346; Sequence no.: 5536; Elapsed time: 5459.49\n",
            "Batch no. 351; Sequence no.: 5616; Elapsed time: 5536.97\n",
            "Batch no. 356; Sequence no.: 5696; Elapsed time: 5614.47\n",
            "Batch no. 361; Sequence no.: 5776; Elapsed time: 5692.51\n",
            "Batch no. 366; Sequence no.: 5856; Elapsed time: 5771.88\n",
            "Batch no. 371; Sequence no.: 5936; Elapsed time: 5851.17\n",
            "Batch no. 376; Sequence no.: 6016; Elapsed time: 5930.69\n",
            "Batch no. 381; Sequence no.: 6096; Elapsed time: 6010.13\n",
            "Batch no. 386; Sequence no.: 6176; Elapsed time: 6089.12\n",
            "Batch no. 391; Sequence no.: 6256; Elapsed time: 6167.71\n",
            "Batch no. 396; Sequence no.: 6336; Elapsed time: 6246.76\n",
            "Batch no. 401; Sequence no.: 6416; Elapsed time: 6325.64\n",
            "Batch no. 406; Sequence no.: 6496; Elapsed time: 6404.48\n",
            "Batch no. 411; Sequence no.: 6576; Elapsed time: 6483.19\n",
            "Batch no. 416; Sequence no.: 6656; Elapsed time: 6562.23\n",
            "Batch no. 421; Sequence no.: 6736; Elapsed time: 6641.70\n",
            "Batch no. 426; Sequence no.: 6816; Elapsed time: 6721.14\n",
            "Batch no. 431; Sequence no.: 6896; Elapsed time: 6800.80\n",
            "Batch no. 436; Sequence no.: 6976; Elapsed time: 6880.05\n",
            "Batch no. 441; Sequence no.: 7056; Elapsed time: 6959.04\n",
            "Batch no. 446; Sequence no.: 7136; Elapsed time: 7037.53\n",
            "Batch no. 451; Sequence no.: 7216; Elapsed time: 7116.06\n",
            "Batch no. 456; Sequence no.: 7296; Elapsed time: 7195.20\n",
            "Batch no. 461; Sequence no.: 7376; Elapsed time: 7274.52\n",
            "Batch no. 466; Sequence no.: 7456; Elapsed time: 7353.55\n",
            "Batch no. 471; Sequence no.: 7536; Elapsed time: 7432.62\n",
            "Batch no. 476; Sequence no.: 7616; Elapsed time: 7510.77\n",
            "Batch no. 481; Sequence no.: 7696; Elapsed time: 7589.92\n",
            "Batch no. 486; Sequence no.: 7776; Elapsed time: 7669.40\n",
            "Batch no. 491; Sequence no.: 7856; Elapsed time: 7748.76\n",
            "Batch no. 496; Sequence no.: 7936; Elapsed time: 7827.43\n",
            "Batch no. 501; Sequence no.: 8016; Elapsed time: 7906.94\n",
            "Batch no. 506; Sequence no.: 8096; Elapsed time: 7986.42\n",
            "Batch no. 511; Sequence no.: 8176; Elapsed time: 8064.47\n",
            "Batch no. 516; Sequence no.: 8256; Elapsed time: 8142.47\n",
            "Batch no. 521; Sequence no.: 8336; Elapsed time: 8220.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oMDDNTGtdGDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}